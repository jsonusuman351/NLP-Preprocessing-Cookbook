{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942efc20-0d52-489e-9172-9d9ea07729a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the Natural Language Toolkit library\n",
    "import nltk\n",
    "# Import PorterStemmer for reducing words to their root/stem form\n",
    "from nltk.stem import PorterStemmer\n",
    "# Import stopwords collection (common words like 'the', 'a', 'an' that are often filtered out)\n",
    "from nltk.corpus import stopwords\n",
    "# Import tokenization functions to split text into sentences and words\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1723247b-7ba7-490c-9cdb-f298168f72e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = '''\n",
    "I am the Ex Co-founder and Chief AI Engineer of iNeuron and my experience is pioneering in\n",
    "machine learning, deep learning, and computer vision, Generative AI, an educator, and a mentor,\n",
    "with over 15 years' experience in the industry. These are my Udemy Courses where I explain\n",
    "various topics on machine learning, deep learning, and AI with many real-world problem\n",
    "scenarios. I have delivered over 30+ tech talks on data science, machine learning, and AI at\n",
    "various meet-ups, technical institutions, and community-arranged forums. My main aim is to\n",
    "make everyone familiar with ML and AI.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b81c330-92dc-45f4-a421-5eb6b4caa4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nI am the Ex Co-founder and Chief AI Engineer of iNeuron and my experience is pioneering in\\nmachine learning, deep learning, and computer vision, Generative AI, an educator, and a mentor,\\nwith over 15 years' experience in the industry. These are my Udemy Courses where I explain\\nvarious topics on machine learning, deep learning, and AI with many real-world problem\\nscenarios. I have delivered over 30+ tech talks on data science, machine learning, and AI at\\nvarious meet-ups, technical institutions, and community-arranged forums. My main aim is to\\nmake everyone familiar with ML and AI.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5518d131-c244-4e21-aaa9-af5b3c2df34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the 'punkt_tab' resource from NLTK\n",
    "# This resource is used for tokenization (breaking text into words, sentences, etc.)\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27b4f6ff-21ff-4148-8f04-649c3836e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the paragraph into sentences using NLTK's sentence tokenizer\n",
    "# This splits the text at sentence boundaries, recognizing punctuation and abbreviations\n",
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b242cfa9-29d3-4028-9bd6-46dcc68894bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"\\nI am the Ex Co-founder and Chief AI Engineer of iNeuron and my experience is pioneering in\\nmachine learning, deep learning, and computer vision, Generative AI, an educator, and a mentor,\\nwith over 15 years' experience in the industry.\", 'These are my Udemy Courses where I explain\\nvarious topics on machine learning, deep learning, and AI with many real-world problem\\nscenarios.', 'I have delivered over 30+ tech talks on data science, machine learning, and AI at\\nvarious meet-ups, technical institutions, and community-arranged forums.', 'My main aim is to\\nmake everyone familiar with ML and AI.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad9e45b5-ea6a-4d6d-b06b-b3699033941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82b9900c-a53c-45ad-8fa8-2a1ea537ed74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('going')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ea2e63d-8851-4685-b239-a3a23235048b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'history'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assistant\n",
    "# First, import the necessary modules\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download the required WordNet resource\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the WordNet lemmatizer from NLTK\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the word 'history' (converts to base/dictionary form)\n",
    "# Default POS is 'noun', so this returns 'history' unchanged\n",
    "lemmatizer.lemmatize('history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "640d1b01-7d45-4e37-8678-da191cbc808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f15e214-4071-465a-a0ec-1e92d903af63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "872d66d1-0934-4c2a-8268-f3be7c5c6d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store processed text\n",
    "corpus = []\n",
    "# Iterate through each sentence in the sentences list\n",
    "for i in range(len(sentences)):\n",
    "    # Remove all non-alphabetic characters and replace with spaces\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    # Convert all characters to lowercase\n",
    "    review = review.lower()\n",
    "    # Add the processed text to the corpus list\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b95c9170-c945-4a6a-86e7-2315ff081e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' i am the ex co founder and chief ai engineer of ineuron and my experience is pioneering in machine learning  deep learning  and computer vision  generative ai  an educator  and a mentor  with over    years  experience in the industry ',\n",
       " 'these are my udemy courses where i explain various topics on machine learning  deep learning  and ai with many real world problem scenarios ',\n",
       " 'i have delivered over     tech talks on data science  machine learning  and ai at various meet ups  technical institutions  and community arranged forums ',\n",
       " 'my main aim is to make everyone familiar with ml and ai ']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34c43fd8-9959-4a2d-a6a6-daa93c346508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c717ae1-499e-40b7-8386-b75117c2a628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ex\n",
      "co\n",
      "founder\n",
      "chief\n",
      "ai\n",
      "engin\n",
      "ineuron\n",
      "experi\n",
      "pioneer\n",
      "machin\n",
      "learn\n",
      "deep\n",
      "learn\n",
      "comput\n",
      "vision\n",
      "gener\n",
      "ai\n",
      "educ\n",
      "mentor\n",
      "year\n",
      "experi\n",
      "industri\n",
      "udemi\n",
      "cours\n",
      "explain\n",
      "variou\n",
      "topic\n",
      "machin\n",
      "learn\n",
      "deep\n",
      "learn\n",
      "ai\n",
      "mani\n",
      "real\n",
      "world\n",
      "problem\n",
      "scenario\n",
      "deliv\n",
      "tech\n",
      "talk\n",
      "data\n",
      "scienc\n",
      "machin\n",
      "learn\n",
      "ai\n",
      "variou\n",
      "meet\n",
      "up\n",
      "technic\n",
      "institut\n",
      "commun\n",
      "arrang\n",
      "forum\n",
      "main\n",
      "aim\n",
      "make\n",
      "everyon\n",
      "familiar\n",
      "ml\n",
      "ai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# First, download the required NLTK resources\n",
    "import nltk\n",
    "nltk.download('stopwords')  # Download stopwords corpus for filtering common words\n",
    "\n",
    "\n",
    "for i in corpus:  # Iterate through each document/text in the corpus\n",
    "    words = nltk.word_tokenize(i)  # Split the text into individual words\n",
    "    for word in words:  # Process each word\n",
    "        if word not in set(stopwords.words('english')):  # Filter out common English stopwords\n",
    "            print(stemmer.stem(word))  # Reduce word to its root form and print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8071f541-0c5b-48ca-80c5-3c8606e6c4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ex\n",
      "co\n",
      "founder\n",
      "chief\n",
      "ai\n",
      "engineer\n",
      "ineuron\n",
      "experience\n",
      "pioneering\n",
      "machine\n",
      "learning\n",
      "deep\n",
      "learning\n",
      "computer\n",
      "vision\n",
      "generative\n",
      "ai\n",
      "educator\n",
      "mentor\n",
      "year\n",
      "experience\n",
      "industry\n",
      "udemy\n",
      "course\n",
      "explain\n",
      "various\n",
      "topic\n",
      "machine\n",
      "learning\n",
      "deep\n",
      "learning\n",
      "ai\n",
      "many\n",
      "real\n",
      "world\n",
      "problem\n",
      "scenario\n",
      "delivered\n",
      "tech\n",
      "talk\n",
      "data\n",
      "science\n",
      "machine\n",
      "learning\n",
      "ai\n",
      "various\n",
      "meet\n",
      "ups\n",
      "technical\n",
      "institution\n",
      "community\n",
      "arranged\n",
      "forum\n",
      "main\n",
      "aim\n",
      "make\n",
      "everyone\n",
      "familiar\n",
      "ml\n",
      "ai\n"
     ]
    }
   ],
   "source": [
    "## lemmatization\n",
    "for i in corpus:  # Iterate through each document/text in the corpus\n",
    "    words = nltk.word_tokenize(i)  # Split the text into individual words\n",
    "    for word in words:  # Process each word\n",
    "        if word not in set(stopwords.words('english')):  # Filter out common English stopwords\n",
    "            print(lemmatizer.lemmatize(word))  # Reduce word to its root form and print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b3896c2-4adc-49b0-89e6-2ed5042b9176",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply stopswords.  lemmatize\n",
    "# Apply Stopwords, Lemmatize\n",
    "import re\n",
    "\n",
    "# Initialize empty list to store processed text\n",
    "corpus = []\n",
    "\n",
    "# Loop through each sentence in the sentences list\n",
    "for i in range(len(sentences)):\n",
    "    # Remove all non-alphabetic characters and replace with spaces\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    # Convert text to lowercase\n",
    "    review = review.lower()\n",
    "    # Split the text into individual words\n",
    "    review = review.split()\n",
    "    # For each word: check if it's not a stopword, then lemmatize it\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    # Join the processed words back into a single string\n",
    "    review = ' '.join(review)\n",
    "    # Add the processed text to our corpus\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "873095c8-add8-4f94-88f4-6792a2bfd6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ex co founder chief ai engineer ineuron experience pioneering machine learning deep learning computer vision generative ai educator mentor year experience industry',\n",
       " 'udemy course explain various topic machine learning deep learning ai many real world problem scenario',\n",
       " 'delivered tech talk data science machine learning ai various meet ups technical institution community arranged forum',\n",
       " 'main aim make everyone familiar ml ai']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9588aff8-2309-406b-ae98-d87a70ec52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer from scikit-learn's text feature extraction module\n",
    "# CountVectorizer converts a collection of text documents to a matrix of token counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffc03792-49aa-4e60-9c07-713f07b4f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer with binary=True option\n",
    "# This transforms text into a binary bag-of-words representation\n",
    "# where feature values are 1 if the word appears in the document, 0 otherwise\n",
    "cv=CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ed6ed5b-536b-424a-8f73-b05d7f157c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the text corpus into a document-term matrix using the fitted CountVectorizer\n",
    "# This converts the text documents into numerical feature vectors based on word frequencies\n",
    "x = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0556a46-2f55-4429-90f4-30c9ae7bc33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ex': 14,\n",
       " 'co': 4,\n",
       " 'founder': 19,\n",
       " 'chief': 3,\n",
       " 'ai': 0,\n",
       " 'engineer': 12,\n",
       " 'ineuron': 22,\n",
       " 'experience': 15,\n",
       " 'pioneering': 32,\n",
       " 'machine': 25,\n",
       " 'learning': 24,\n",
       " 'deep': 9,\n",
       " 'computer': 6,\n",
       " 'vision': 44,\n",
       " 'generative': 20,\n",
       " 'educator': 11,\n",
       " 'mentor': 30,\n",
       " 'year': 46,\n",
       " 'industry': 21,\n",
       " 'udemy': 41,\n",
       " 'course': 7,\n",
       " 'explain': 16,\n",
       " 'various': 43,\n",
       " 'topic': 40,\n",
       " 'many': 28,\n",
       " 'real': 34,\n",
       " 'world': 45,\n",
       " 'problem': 33,\n",
       " 'scenario': 35,\n",
       " 'delivered': 10,\n",
       " 'tech': 38,\n",
       " 'talk': 37,\n",
       " 'data': 8,\n",
       " 'science': 36,\n",
       " 'meet': 29,\n",
       " 'ups': 42,\n",
       " 'technical': 39,\n",
       " 'institution': 23,\n",
       " 'community': 5,\n",
       " 'arranged': 2,\n",
       " 'forum': 18,\n",
       " 'main': 26,\n",
       " 'aim': 1,\n",
       " 'make': 27,\n",
       " 'everyone': 13,\n",
       " 'familiar': 17,\n",
       " 'ml': 31}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the vocabulary_ attribute of the CountVectorizer (cv) object\n",
    "# This returns a dictionary mapping terms to their indices in the feature matrix\n",
    "# The keys are the unique words found in the corpus, and the values are their integer indices\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79410e5d-dafb-4c03-8111-2a86ce01366d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ex co founder chief ai engineer ineuron experience pioneering machine learning deep learning computer vision generative ai educator mentor year experience industry'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca3ddcfc-6e3c-4b65-9052-8f7a2cbbcd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the sparse matrix at index 0 of x to a dense numpy array\n",
    "x[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca768a-29bd-43c7-b377-f243d0986556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e146fa-8ede-4f20-ad5a-786238c0291b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
